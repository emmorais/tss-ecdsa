//! Types and functions related to the key refresh sub-protocol Participant.

// Copyright (c) Facebook, Inc. and its affiliates.
// Modifications Copyright (c) 2022-2023 Bolt Labs Holdings, Inc
//
// This source code is licensed under both the MIT license found in the
// LICENSE-MIT file in the root directory of this source tree and the Apache
// License, Version 2.0 found in the LICENSE-APACHE file in the root directory
// of this source tree.

use crate::{
    broadcast::participant::{BroadcastOutput, BroadcastParticipant, BroadcastTag},
    curve::CurveTrait,
    errors::{CallerError, InternalError, Result},
    keyrefresh::{
        keyrefresh_commit::{KeyrefreshCommit, KeyrefreshDecommit},
        keyshare::{KeyUpdateEncrypted, KeyUpdatePrivate, KeyUpdatePublic},
    },
    local_storage::LocalStorage,
    messages::{KeyrefreshMessageType, Message, MessageType},
    participant::{
        Broadcast, InnerProtocolParticipant, ProcessOutcome, ProtocolParticipant, Status,
    },
    protocol::{ParticipantIdentifier, ProtocolType, SharedContext},
    run_only_once,
    zkp::pisch::{CommonInput, PiSchPrecommit, PiSchProof, ProverSecret},
    Identifier, ParticipantConfig,
};

use merlin::Transcript;
use rand::{CryptoRng, RngCore};
use tracing::{error, info, instrument, warn};

use super::{input::Input, Output};

mod storage {
    use std::marker::PhantomData;

    use super::*;
    use crate::local_storage::TypeTag;

    pub(super) struct Commit;
    impl TypeTag for Commit {
        type Value = KeyrefreshCommit;
    }
    pub(super) struct Decommit<C>(PhantomData<C>);
    impl<C: Send + Sync + 'static> TypeTag for Decommit<C> {
        type Value = KeyrefreshDecommit<C>;
    }
    pub(super) struct VecSchnorrPrecom<C>(PhantomData<C>);
    impl<C: Send + Sync + 'static> TypeTag for VecSchnorrPrecom<C> {
        type Value = Vec<PiSchPrecommit<C>>;
    }
    pub(super) struct GlobalRid;
    impl TypeTag for GlobalRid {
        type Value = [u8; 32];
    }
    pub(super) struct PrivateUpdatesForOthers<C>(PhantomData<C>);
    impl<C: Send + Sync + 'static> TypeTag for PrivateUpdatesForOthers<C> {
        type Value = Vec<super::KeyUpdatePrivate<C>>;
    }
    pub(super) struct ValidPublicUpdates<C>(PhantomData<C>);
    impl<C: Send + Sync + 'static> TypeTag for ValidPublicUpdates<C> {
        type Value = Vec<super::KeyUpdatePublic<C>>;
    }
    pub(super) struct ValidPrivateUpdate<C>(PhantomData<C>);
    impl<C: Send + Sync + 'static> TypeTag for ValidPrivateUpdate<C> {
        type Value = super::KeyUpdatePrivate<C>;
    }
}

/**
A [`ProtocolParticipant`] that runs the key refresh protocol.

# Protocol input
- Encryption and decryption keys from the Aux-Info protocol.

# Protocol output
Upon successful completion, the participant produces [`Output`], which
includes:
- A list of *updated* public key shares, one for each participant (including this
  participant);
- A single *updated* private key share for this participant.

This is the same output structure as the keygen protocol.
# ðŸ”’ Storage requirements
The [private key share](crate::keygen::KeySharePrivate) in the output requires secure
persistent storage.
**/
#[derive(Debug)]
pub struct KeyrefreshParticipant<C: CurveTrait> {
    /// The current session identifier
    sid: Identifier,
    /// The current protocol input.
    input: Input<C>,
    /// A unique identifier for this participant.
    id: ParticipantIdentifier,
    /// A list of all other participant identifiers participating in the
    /// protocol
    other_participant_ids: Vec<ParticipantIdentifier>,
    /// Local storage for this participant to store secrets
    local_storage: LocalStorage,
    /// Broadcast subprotocol handler
    broadcast_participant: BroadcastParticipant<C>,
    /// Status of the protocol execution.
    status: Status,
}

impl<C: CurveTrait> ProtocolParticipant for KeyrefreshParticipant<C> {
    type Input = Input<C>;
    type Output = Output<C>;

    fn new(
        sid: Identifier,
        id: ParticipantIdentifier,
        other_participant_ids: Vec<ParticipantIdentifier>,
        input: Self::Input,
    ) -> Result<Self> {
        input.check_participant_config(&ParticipantConfig::new(id, &other_participant_ids)?)?;

        Ok(Self {
            sid,
            input,
            id,
            other_participant_ids: other_participant_ids.clone(),
            local_storage: Default::default(),
            broadcast_participant: BroadcastParticipant::new(sid, id, other_participant_ids, ())?,
            status: Status::NotReady,
        })
    }

    fn ready_type() -> MessageType {
        MessageType::Keyrefresh(KeyrefreshMessageType::Ready)
    }

    fn protocol_type() -> ProtocolType {
        ProtocolType::Keyrefresh
    }

    fn id(&self) -> ParticipantIdentifier {
        self.id
    }

    fn other_ids(&self) -> &[ParticipantIdentifier] {
        &self.other_participant_ids
    }

    fn sid(&self) -> Identifier {
        self.sid
    }

    #[cfg_attr(feature = "flame_it", flame("keyrefresh"))]
    #[instrument(skip_all)]
    fn process_message<R: RngCore + CryptoRng>(
        &mut self,
        rng: &mut R,
        message: &Message,
    ) -> Result<ProcessOutcome<Self::Output>> {
        info!(
            "KEYREFRESH: Player {}: received {:?} from {}",
            self.id(),
            message.message_type(),
            message.from()
        );

        if *self.status() == Status::TerminatedSuccessfully {
            Err(CallerError::ProtocolAlreadyTerminated)?;
        }

        if !self.status().is_ready() && message.message_type() != Self::ready_type() {
            self.stash_message(message)?;
            return Ok(ProcessOutcome::Incomplete);
        }

        match message.message_type() {
            MessageType::Keyrefresh(KeyrefreshMessageType::Ready) => {
                self.handle_ready_msg(rng, message)
            }
            MessageType::Keyrefresh(KeyrefreshMessageType::R1CommitHash) => {
                let broadcast_outcome = self.handle_broadcast(rng, message)?;

                // Handle the broadcasted message if all parties have agreed on it
                broadcast_outcome.convert(self, Self::handle_round_one_msg, rng)
            }
            MessageType::Keyrefresh(KeyrefreshMessageType::R2Decommit) => {
                self.handle_round_two_msg(rng, message)
            }
            MessageType::Keyrefresh(KeyrefreshMessageType::R3Proofs) => {
                self.handle_round_three_msg(message)
            }
            MessageType::Keyrefresh(KeyrefreshMessageType::R3PrivateUpdate) => {
                self.handle_round_three_msg_private(message)
            }
            message_type => {
                error!(
                    "Incorrect MessageType given to KeyrefreshParticipant. Got: {:?}",
                    message_type
                );
                Err(InternalError::InternalInvariantFailed)
            }
        }
    }

    fn status(&self) -> &Status {
        &self.status
    }
}

impl<C: CurveTrait> InnerProtocolParticipant for KeyrefreshParticipant<C> {
    type Context = SharedContext<C>;

    fn retrieve_context(&self) -> <Self as InnerProtocolParticipant>::Context {
        SharedContext::collect(self)
    }

    fn local_storage(&self) -> &LocalStorage {
        &self.local_storage
    }

    fn local_storage_mut(&mut self) -> &mut LocalStorage {
        &mut self.local_storage
    }

    fn status_mut(&mut self) -> &mut Status {
        &mut self.status
    }
}

impl<C: CurveTrait> Broadcast<C> for KeyrefreshParticipant<C> {
    fn broadcast_participant(&mut self) -> &mut BroadcastParticipant<C> {
        &mut self.broadcast_participant
    }
}

impl<C: CurveTrait> KeyrefreshParticipant<C> {
    /// Handle "Ready" messages from the protocol participants.
    ///
    /// Once "Ready" messages have been received from all participants, this
    /// method will trigger this participant to generate its round one message.
    #[cfg_attr(feature = "flame_it", flame("keyrefresh"))]
    #[instrument(skip_all, err(Debug))]
    fn handle_ready_msg<R: RngCore + CryptoRng>(
        &mut self,
        rng: &mut R,
        message: &Message,
    ) -> Result<ProcessOutcome<<Self as ProtocolParticipant>::Output>> {
        info!("Handling ready keyrefresh message.");

        let ready_outcome = self.process_ready_message(rng, message)?;
        let round_one_messages = run_only_once!(self.gen_round_one_msgs(rng, message.id()))?;
        // extend the output with r1 messages (if they hadn't already been generated)
        Ok(ready_outcome.with_messages(round_one_messages))
    }

    /// Generate the protocol's round one message.
    ///
    /// The outcome is a broadcast message containing a commitment to:
    /// - updates [`KeyUpdatePublic`] X_ij for all other participants,
    /// - "pre-commitments" A_ij for Schnorr proofs.
    #[cfg_attr(feature = "flame_it", flame("keyrefresh"))]
    #[instrument(skip_all, err(Debug))]
    fn gen_round_one_msgs<R: RngCore + CryptoRng>(
        &mut self,
        rng: &mut R,
        sid: Identifier,
    ) -> Result<Vec<Message>> {
        info!("Generating round one keyrefresh messages.");

        // Generate keyshare updates for all participants.
        let (update_privates, update_publics) = {
            let mut privates = vec![];
            let mut publics = vec![];

            // Random shares for others.
            for pid in self.other_ids() {
                let (private, public) = KeyUpdatePublic::new_keyshare(*pid, rng)?;
                privates.push(private);
                publics.push(public);
            }

            // Compute one's own update such that the sum of updates is 0.
            let my_update_private = KeyUpdatePrivate::zero_sum(&privates);
            let my_update_public =
                KeyUpdatePublic::new(self.id(), my_update_private.public_point()?);
            privates.push(my_update_private);
            publics.push(my_update_public);

            (privates, publics)
        };

        // This corresponds to `A_ij` in the paper.
        let sch_precoms = (0..update_publics.len())
            .map(|_| PiSchProof::precommit(rng))
            .collect::<Result<Vec<_>>>()?;

        let decom = KeyrefreshDecommit::new(
            rng,
            &sid,
            &self.id(),
            update_publics,
            sch_precoms
                .iter()
                .map(|sch_precom| *sch_precom.precommitment())
                .collect(),
        );

        // Store the beginning of our proofs so we can continue the proofs later.
        self.local_storage
            .store::<storage::VecSchnorrPrecom<C>>(self.id(), sch_precoms);

        // Mark our own public updates as verified.
        self.local_storage
            .store::<storage::ValidPublicUpdates<C>>(self.id(), decom.update_publics.clone());

        // Store the private update from ourselves to ourselves.
        self.local_storage.store::<storage::ValidPrivateUpdate<C>>(
            self.id(),
            update_privates[update_privates.len() - 1].clone(),
        );

        // Store the private updates from us to others so we can share them later.
        self.local_storage
            .store::<storage::PrivateUpdatesForOthers<C>>(self.id(), update_privates);

        // Mark our own commitment as ready. This corresponds to `V_i` in the paper.
        let com = decom.commit()?;
        let com_bytes = serialize!(&com)?;
        self.local_storage.store::<storage::Commit>(self.id(), com);

        // Store our committed values so we can open the commitment later.
        self.local_storage
            .store::<storage::Decommit<C>>(self.id(), decom);

        let messages = self.broadcast(
            rng,
            MessageType::Keyrefresh(KeyrefreshMessageType::R1CommitHash),
            com_bytes,
            sid,
            BroadcastTag::KeyRefreshR1CommitHash,
        )?;
        Ok(messages)
    }

    /// Handle round one messages from the protocol participants.
    ///
    /// In round one, each participant broadcasts its commitment to its public
    /// key share and a "precommitment" to a Schnorr proof. Once all such
    /// commitments have been received, this participant will send an opening of
    /// its own commitment to all other parties.
    #[cfg_attr(feature = "flame_it", flame("keyrefresh"))]
    #[instrument(skip_all, err(Debug))]
    fn handle_round_one_msg<R: RngCore + CryptoRng>(
        &mut self,
        rng: &mut R,
        broadcast_message: BroadcastOutput,
    ) -> Result<ProcessOutcome<<Self as ProtocolParticipant>::Output>> {
        let message = broadcast_message.into_message(BroadcastTag::KeyRefreshR1CommitHash)?;

        self.check_for_duplicate_msg::<storage::Commit>(message.from())?;
        info!("Handling round one keyrefresh message.");

        let keyrefresh_commit = KeyrefreshCommit::from_message(&message)?;
        self.local_storage
            .store_once::<storage::Commit>(message.from(), keyrefresh_commit)?;

        // Check if we've received all the commits, which signals an end to
        // round one.
        //
        // Note: This does _not_ check `self.all_participants` on purpose. There
        // could be a setting where we've received all the round one messages
        // from all other participants, yet haven't ourselves generated our
        // round one message. If we switched to `self.all_participants` here
        // then the result would be `false`, causing the execution to hang.
        //
        // The "right" solution would be to only process the message once the
        // "Ready" round is complete, and stashing messages if it is not yet
        // complete (a la how we do it in `handle_round_two_msg`).
        // Unfortunately, this does not work given the current API because we
        // are dealing with a [`BroadcastOutput`] type instead of a [`Message`]
        // type.
        let r1_done = self
            .local_storage
            .contains_for_all_ids::<storage::Commit>(self.other_ids());

        if r1_done {
            // Finish round 1 by generating messages for round 2
            let round_one_messages = run_only_once!(self.gen_round_two_msgs(rng, message.id()))?;

            // Process any round 2 messages we may have received early
            let round_two_outcomes = self
                .fetch_messages(MessageType::Keyrefresh(KeyrefreshMessageType::R2Decommit))?
                .iter()
                .map(|msg| self.handle_round_two_msg(rng, msg))
                .collect::<Result<Vec<_>>>()?;

            ProcessOutcome::collect_with_messages(round_two_outcomes, round_one_messages)
        } else {
            // Otherwise, wait for more round 1 messages
            Ok(ProcessOutcome::Incomplete)
        }
    }

    /// Generate the protocol's round two messages.
    ///
    /// The outcome is an opening to the commitment generated in round one.
    #[cfg_attr(feature = "flame_it", flame("keyrefresh"))]
    #[instrument(skip_all, err(Debug))]
    fn gen_round_two_msgs<R: RngCore + CryptoRng>(
        &mut self,
        rng: &mut R,
        sid: Identifier,
    ) -> Result<Vec<Message>> {
        info!("Generating round two keyrefresh messages.");

        let mut messages = vec![];
        // Check that we've generated our keyshare before trying to retrieve it.
        //
        // Because we are not checking `self.all_participants` in
        // `handle_round_one_msg`, we may reach this point and not actually have
        // generated round one messages for ourselves (in particular,
        // `PublicKeyshare` and `Decommit`). This check forces that behavior.
        // Without it we'll get a `InternalInvariantFailed` error when trying to
        // retrieve `Decommit` below.
        if !self
            .local_storage
            .contains::<storage::Decommit<C>>(self.id())
        {
            let more_messages = run_only_once!(self.gen_round_one_msgs(rng, sid))?;
            messages.extend_from_slice(&more_messages);
        }

        let decom = self
            .local_storage
            .retrieve::<storage::Decommit<C>>(self.id())?;
        let more_messages = self.message_for_other_participants(
            MessageType::Keyrefresh(KeyrefreshMessageType::R2Decommit),
            decom,
        )?;
        messages.extend_from_slice(&more_messages);
        Ok(messages)
    }

    /// Handle the protocol's round two messages.
    ///
    /// Here we check that the decommitments from each participant are valid.
    #[cfg_attr(feature = "flame_it", flame("keyrefresh"))]
    #[instrument(skip_all, err(Debug))]
    fn handle_round_two_msg<R: RngCore + CryptoRng>(
        &mut self,
        rng: &mut R,
        message: &Message,
    ) -> Result<ProcessOutcome<<Self as ProtocolParticipant>::Output>> {
        self.check_for_duplicate_msg::<storage::Decommit<C>>(message.from())?;
        info!("Handling round two keyrefresh message.");

        // We must receive all commitments in round 1 before we start processing
        // decommits in round 2.
        let r1_done = self
            .local_storage
            .contains_for_all_ids::<storage::Commit>(&self.all_participants());
        if !r1_done {
            // Store any early round 2 messages
            self.stash_message(message)?;
            return Ok(ProcessOutcome::Incomplete);
        }
        // Check that the decommitment contained in the message is valid against
        // the previously received commitment and protocol rules.
        let com = self
            .local_storage
            .retrieve::<storage::Commit>(message.from())?;
        let decom = KeyrefreshDecommit::from_message(message, com, &self.all_participants())?;
        self.local_storage
            .store_once::<storage::Decommit<C>>(message.from(), decom)?;

        // Check if we've received all the decommits
        let r2_done = self
            .local_storage
            .contains_for_all_ids::<storage::Decommit<C>>(&self.all_participants());

        if r2_done {
            // Generate messages for round 3...
            let round_three_messages = run_only_once!(self.gen_round_three_msgs(rng))?;

            // ...and handle any messages that other participants have sent for round 3.
            let mut round_three_outcomes = self
                .fetch_messages(MessageType::Keyrefresh(KeyrefreshMessageType::R3Proofs))?
                .iter()
                .map(|msg| self.handle_round_three_msg(msg))
                .collect::<Result<Vec<_>>>()?;

            let outcomes_private = self
                .fetch_messages(MessageType::Keyrefresh(
                    KeyrefreshMessageType::R3PrivateUpdate,
                ))?
                .iter()
                .map(|msg| self.handle_round_three_msg_private(msg))
                .collect::<Result<Vec<_>>>()?;
            round_three_outcomes.extend(outcomes_private);

            ProcessOutcome::collect_with_messages(round_three_outcomes, round_three_messages)
        } else {
            // Otherwise, wait for more round 2 messages.
            Ok(ProcessOutcome::Incomplete)
        }
    }

    /// Generate the protocol's round three messages.
    ///
    /// At this point, we have validated each participant's commitment, and can
    /// now proceed to constructing a Schnorr proof that this participant knows
    /// the private value corresponding to its public key share.
    #[cfg_attr(feature = "flame_it", flame("keyrefresh"))]
    #[instrument(skip_all, err(Debug))]
    fn gen_round_three_msgs<R: RngCore + CryptoRng>(
        &mut self,
        rng: &mut R,
    ) -> Result<Vec<Message>> {
        info!("Generating round three keyrefresh messages.");

        // Construct `global rid` out of each participant's `rid`s.
        let my_rid = self
            .local_storage
            .retrieve::<storage::Decommit<C>>(self.id())?
            .rid;
        let rids: Vec<[u8; 32]> = self
            .other_ids()
            .iter()
            .map(|&other_participant_id| {
                let decom = self
                    .local_storage
                    .retrieve::<storage::Decommit<C>>(other_participant_id)?;
                Ok(decom.rid)
            })
            .collect::<Result<Vec<[u8; 32]>>>()?;
        let mut global_rid = my_rid;
        // xor all the rids together.
        for rid in rids.iter() {
            for i in 0..32 {
                global_rid[i] ^= rid[i];
            }
        }
        self.local_storage
            .store::<storage::GlobalRid>(self.id(), global_rid);

        let decom = self
            .local_storage
            .retrieve::<storage::Decommit<C>>(self.id())?;

        let transcript = schnorr_proof_transcript(self.sid(), &global_rid, self.id())?;

        // Generate proofs for each update.
        let precoms = self
            .local_storage
            .retrieve::<storage::VecSchnorrPrecom<C>>(self.id())?;

        let private_updates = self
            .local_storage
            .retrieve::<storage::PrivateUpdatesForOthers<C>>(self.id())?;

        let mut proofs: Vec<PiSchProof<C>> = vec![];
        for i in 0..precoms.len() {
            let pk = &decom.update_publics[i];
            let input = CommonInput::<C>::new(pk);
            let precom = &precoms[i];
            let sk = &private_updates[i];

            let proof = PiSchProof::prove_from_precommit(
                &self.retrieve_context(),
                precom,
                &input,
                &ProverSecret::new(sk.as_ref()),
                &transcript,
            )?;

            proofs.push(proof);
        }

        // Encrypt the private updates to each participant.
        let encrypted_updates = self
            .other_ids()
            .iter()
            .zip(private_updates.iter())
            .map(|(other_participant_id, update_private)| {
                let auxinfo = self.input.find_auxinfo_public(*other_participant_id)?;
                KeyUpdateEncrypted::encrypt(update_private, auxinfo.pk(), rng)
            })
            .collect::<Result<Vec<_>>>()?;

        // Send all proofs to everybody.
        let mut messages = self.message_for_other_participants(
            MessageType::Keyrefresh(KeyrefreshMessageType::R3Proofs),
            proofs,
        )?;

        // Send their private updates to each individual participant.
        messages.extend(
            self.other_ids()
                .iter()
                .zip(encrypted_updates.iter())
                .map(|(other_participant_id, encrypted_update)| {
                    Message::new(
                        MessageType::Keyrefresh(KeyrefreshMessageType::R3PrivateUpdate),
                        self.sid(),
                        self.id(),
                        *other_participant_id,
                        encrypted_update,
                    )
                })
                .collect::<Result<Vec<Message>>>()?,
        );

        Ok(messages)
    }

    /// Handle round three messages only after our own `gen_round_three_msgs`.
    fn can_handle_round_three_msg(&self) -> bool {
        self.local_storage.contains::<storage::GlobalRid>(self.id())
    }

    /// Handle the protocol's round three public messages.
    ///
    /// Here we validate the Schnorr proofs from each participant.
    #[cfg_attr(feature = "flame_it", flame("keyrefresh"))]
    #[instrument(skip_all, err(Debug))]
    fn handle_round_three_msg(
        &mut self,
        message: &Message,
    ) -> Result<ProcessOutcome<<Self as ProtocolParticipant>::Output>> {
        self.check_for_duplicate_msg::<storage::ValidPublicUpdates<C>>(message.from())?;

        if !self.can_handle_round_three_msg() {
            info!("Not yet ready to handle round three keyrefresh broadcast message.");
            self.stash_message(message)?;
            return Ok(ProcessOutcome::Incomplete);
        }
        info!("Handling round three keyrefresh broadcast message.");

        let global_rid = *self
            .local_storage
            .retrieve::<storage::GlobalRid>(self.id())?;

        let proofs = PiSchProof::from_message_multi(message)?;
        let decom = self
            .local_storage
            .retrieve::<storage::Decommit<C>>(message.from())?;

        // Check that there is one proof per participant.
        if proofs.len() != self.all_participants().len() {
            error!("Received incorrect number of proofs",);
            return Err(InternalError::ProtocolError(Some(message.from())));
        }

        for ((proof, precommit), update_public) in proofs
            .into_iter()
            .zip(decom.As.iter())
            .zip(decom.update_publics.iter())
        {
            let mut transcript = schnorr_proof_transcript(self.sid(), &global_rid, message.from())?;
            proof.verify_with_precommit(
                CommonInput::<C>::new(update_public),
                &self.retrieve_context(),
                &mut transcript,
                precommit,
            )?;
        }

        // Only if the proof verifies do we store the participant's updates.
        self.local_storage
            .store_once::<storage::ValidPublicUpdates<C>>(
                message.from(),
                decom.update_publics.clone(),
            )?;

        self.maybe_finish()
    }

    /// Handle the protocol's round three private messages.
    ///
    /// Here we validate and store a private update from someone to us.
    #[cfg_attr(feature = "flame_it", flame("keyrefresh"))]
    #[instrument(skip_all, err(Debug))]
    fn handle_round_three_msg_private(
        &mut self,
        message: &Message,
    ) -> Result<ProcessOutcome<<Self as ProtocolParticipant>::Output>> {
        self.check_for_duplicate_msg::<storage::ValidPrivateUpdate<C>>(message.from())?;

        if !self.can_handle_round_three_msg() {
            info!("Not yet ready to handle round three keyrefresh private message.");
            self.stash_message(message)?;
            return Ok(ProcessOutcome::Incomplete);
        }
        info!("Handling round three keyrefresh private message.");

        message.check_type(MessageType::Keyrefresh(
            KeyrefreshMessageType::R3PrivateUpdate,
        ))?;
        let encrypted_update: KeyUpdateEncrypted<C> = deserialize!(&message.unverified_bytes)?;

        // Get my private key from the AuxInfo protocol.
        let my_dk = self.input.private_auxinfo().decryption_key();

        // Decrypt the private update.
        let update_private = encrypted_update.decrypt(my_dk)?;

        // Check that this private share matches our public share in KeyrefreshDecommit
        // from this participant.
        let implied_public = update_private.public_point()?;
        let decom = self
            .local_storage
            .retrieve::<storage::Decommit<C>>(message.from())?;
        let expected_public = decom
            .update_publics
            .iter()
            .find(|kup| kup.participant() == self.id())
            .ok_or(InternalError::InternalInvariantFailed)?
            .as_ref();
        if implied_public != *expected_public {
            error!("the private update does not match the public update");
            return Err(InternalError::ProtocolError(Some(message.from())));
        }

        self.local_storage
            .store::<storage::ValidPrivateUpdate<C>>(message.from(), update_private);

        self.maybe_finish()
    }

    fn maybe_finish(&mut self) -> Result<ProcessOutcome<<Self as ProtocolParticipant>::Output>> {
        // Have we validated and stored the public updates from everybody to everybody?
        let got_all_public_updates = self
            .local_storage
            .contains_for_all_ids::<storage::ValidPublicUpdates<C>>(&self.all_participants());

        // Have we got the private updates from everybody to us?
        let got_all_private_updates = self
            .local_storage
            .contains_for_all_ids::<storage::ValidPrivateUpdate<C>>(&self.all_participants());

        // If so, we completed the protocol! Return the outputs.
        if got_all_public_updates && got_all_private_updates {
            // Compute the update to the public key share of each participant.
            let from_all_to_all_public = self
                .all_participants()
                .iter()
                .map(|pid| {
                    self.local_storage
                        .remove::<storage::ValidPublicUpdates<C>>(*pid)
                })
                .collect::<Result<Vec<_>>>()?;
            let all_public_updates =
                Self::aggregate_public_updates(&self.all_participants(), &from_all_to_all_public)?;

            // Apply the updates to everybody's public key shares.
            let new_public_shares = self
                .input
                .public_key_shares()
                .iter()
                .map(|current_pk| {
                    let pk_update = all_public_updates
                        .iter()
                        .find(|update| update.participant() == current_pk.participant())
                        .ok_or(InternalError::InternalInvariantFailed)?;

                    Ok(pk_update.apply(current_pk))
                })
                .collect::<Result<Vec<_>>>()?;

            // Compute the update to one's own private key share.
            let from_all_to_me_private = self
                .all_participants()
                .iter()
                .map(|pid| {
                    self.local_storage
                        .remove::<storage::ValidPrivateUpdate<C>>(*pid)
                })
                .collect::<Result<Vec<_>>>()?;
            let my_private_update = Self::aggregate_private_updates(&from_all_to_me_private);

            // Apply the update to my private key share.
            let my_new_share = my_private_update.apply(self.input.private_key_share());

            // Return the output and stop.
            let rid_before = *self.input.keygen_output().rid();
            let chain_code_before = *self.input.keygen_output().chain_code();
            let output = Output::from_parts(
                new_public_shares,
                my_new_share,
                rid_before,
                chain_code_before,
            )?;

            // Final validation that the public key of the quorum has not changed. This
            // should never fail because this is already verified in round 2 by
            // `KeyrefreshDecommit::from_message`.
            let pk_before = self.input.keygen_output().public_key()?;
            let pk_after = output.public_key()?;
            if pk_before != pk_after {
                error!("Keyrefresh output did not preserve the quorum public key.");
                return Err(InternalError::InternalInvariantFailed);
            }

            self.status = Status::TerminatedSuccessfully;
            Ok(ProcessOutcome::Terminated(output))
        } else {
            // Otherwise, we'll have to wait for more round three messages.
            Ok(ProcessOutcome::Incomplete)
        }
    }

    fn aggregate_private_updates(update_privates: &[KeyUpdatePrivate<C>]) -> KeyUpdatePrivate<C> {
        KeyUpdatePrivate::sum(update_privates)
    }

    fn aggregate_public_updates(
        participants: &[ParticipantIdentifier],
        from_all_to_all: &[Vec<KeyUpdatePublic<C>>],
    ) -> Result<Vec<KeyUpdatePublic<C>>> {
        participants
            .iter()
            .map(|p_i| {
                let from_all_to_i = Self::find_updates_for_participant(*p_i, from_all_to_all)?;

                let sum_for_i = KeyUpdatePublic::sum(*p_i, &from_all_to_i);
                Ok(sum_for_i)
            })
            .collect()
    }

    fn find_updates_for_participant(
        p_i: ParticipantIdentifier,
        from_all_to_all: &[Vec<KeyUpdatePublic<C>>],
    ) -> Result<Vec<KeyUpdatePublic<C>>> {
        let from_all_to_i = from_all_to_all
            .iter()
            .map(|from_j_to_all| {
                let from_j_to_i = from_j_to_all
                    .iter()
                    .find(|from_j_to_x| from_j_to_x.participant() == p_i)
                    .cloned()
                    .ok_or(InternalError::InternalInvariantFailed);
                from_j_to_i
            })
            .collect();
        from_all_to_i
    }
}

/// Generate a [`Transcript`] for [`PiSchProof`].
fn schnorr_proof_transcript(
    sid: Identifier,
    global_rid: &[u8; 32],
    sender_id: ParticipantIdentifier,
) -> Result<Transcript> {
    let mut transcript = Transcript::new(b"keyrefresh schnorr");
    transcript.append_message(b"sid", &serialize!(&sid)?);
    transcript.append_message(b"rid", &serialize!(global_rid)?);
    transcript.append_message(b"sender_id", &serialize!(&sender_id)?);
    Ok(transcript)
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::{
        auxinfo, curve::TestCurve, keygen, keyrefresh::input::Input, utils::testing::init_testing,
        Identifier, ParticipantConfig,
    };
    use rand::{CryptoRng, Rng, RngCore};
    use std::{
        collections::{HashMap, HashSet},
        iter::zip,
    };
    use tracing::debug;

    impl<C: CurveTrait> KeyrefreshParticipant<C> {
        /// Create a new quorum of participants with random inputs.
        pub fn new_quorum<R: RngCore + CryptoRng>(
            sid: Identifier,
            quorum_size: usize,
            rng: &mut R,
        ) -> Result<Vec<Self>> {
            // Prepare prereqs for making KeyRefreshParticipant's. Assume all the
            // simulations are stable (e.g. keep config order)
            let configs = ParticipantConfig::random_quorum(quorum_size, rng)?;
            let keygen_outputs = keygen::Output::simulate_set(&configs, rng);
            let auxinfo_outputs = auxinfo::Output::simulate_set(&configs, rng);

            // Make the participants
            zip(configs, zip(keygen_outputs.clone(), auxinfo_outputs))
                .map(|(config, (keygen_output, auxinfo_output))| {
                    let input = Input::<C>::new(auxinfo_output, keygen_output)?;
                    Self::new(sid, config.id(), config.other_ids().to_vec(), input)
                })
                .collect::<Result<Vec<_>>>()
        }

        /// Create the initial "Ready" message to start the protocol.
        pub fn initialize_keyrefresh_message(
            &self,
            keyrefresh_identifier: Identifier,
        ) -> Result<Message> {
            let empty: [u8; 0] = [];
            Message::new(
                MessageType::Keyrefresh(KeyrefreshMessageType::Ready),
                keyrefresh_identifier,
                self.id(),
                self.id(),
                &empty,
            )
        }
    }

    /// Delivers all messages into their respective participant's inboxes.
    fn deliver_all(
        messages: &[Message],
        inboxes: &mut HashMap<ParticipantIdentifier, Vec<Message>>,
    ) {
        for message in messages {
            inboxes
                .get_mut(&message.to())
                .unwrap()
                .push(message.clone());
        }
    }

    fn is_keyrefresh_done(quorum: &[KeyrefreshParticipant<TestCurve>]) -> bool {
        for participant in quorum {
            if *participant.status() != Status::TerminatedSuccessfully {
                return false;
            }
        }
        true
    }

    #[allow(clippy::type_complexity)]
    fn process_messages<R: RngCore + CryptoRng>(
        quorum: &mut [KeyrefreshParticipant<TestCurve>],
        inboxes: &mut HashMap<ParticipantIdentifier, Vec<Message>>,
        rng: &mut R,
    ) -> Option<(usize, ProcessOutcome<Output<TestCurve>>)> {
        // Pick a random participant to process
        let index = rng.gen_range(0..quorum.len());
        let participant = quorum.get_mut(index).unwrap();

        let inbox = inboxes.get_mut(&participant.id()).unwrap();
        if inbox.is_empty() {
            // No messages to process for this participant, so pick another participant
            return None;
        }
        let message = inbox.remove(rng.gen_range(0..inbox.len()));
        debug!(
            "processing participant: {}, with message type: {:?} from {}",
            &participant.id(),
            &message.message_type(),
            &message.from(),
        );
        Some((index, participant.process_message(rng, &message).unwrap()))
    }

    #[cfg_attr(feature = "flame_it", flame)]
    #[test]
    fn keyrefresh_always_produces_valid_outputs() -> Result<()> {
        for size in 2..12 {
            keyrefresh_produces_valid_outputs(size)?;
        }
        Ok(())
    }

    fn keyrefresh_produces_valid_outputs(quorum_size: usize) -> Result<()> {
        let mut rng = init_testing();
        let sid = Identifier::random(&mut rng);
        let mut quorum = KeyrefreshParticipant::new_quorum(sid, quorum_size, &mut rng)?;
        let mut inboxes = HashMap::new();
        for participant in &quorum {
            let _ = inboxes.insert(participant.id(), vec![]);
        }

        let inputs = quorum.iter().map(|p| p.input.clone()).collect::<Vec<_>>();

        let mut outputs = std::iter::repeat_with(|| None)
            .take(quorum_size)
            .collect::<Vec<Option<Output<TestCurve>>>>();

        for participant in &quorum {
            let inbox = inboxes.get_mut(&participant.id()).unwrap();
            inbox.push(participant.initialize_keyrefresh_message(sid)?);
        }

        while !is_keyrefresh_done(&quorum) {
            let (index, outcome) = match process_messages(&mut quorum, &mut inboxes, &mut rng) {
                None => continue,
                Some(x) => x,
            };

            // Deliver messages and save outputs
            match outcome {
                ProcessOutcome::Incomplete => {}
                ProcessOutcome::Processed(messages) => deliver_all(&messages, &mut inboxes),
                ProcessOutcome::Terminated(output) => outputs[index] = Some(output),
                ProcessOutcome::TerminatedForThisParticipant(output, messages) => {
                    deliver_all(&messages, &mut inboxes);
                    outputs[index] = Some(output);
                }
            }
        }

        // Make sure every player got an output
        let outputs: Vec<_> = outputs.into_iter().flatten().collect();
        assert_eq!(outputs.len(), quorum_size);

        // Check returned outputs
        //
        // Every participant should have a public output from every other participant
        // and, for a given participant, they should be the same in every output
        for party in quorum.iter_mut() {
            let pid = party.id();

            // Collect the KeySharePublic associated with pid from every output
            let mut publics_for_pid = vec![];
            for output in &outputs {
                let key_share = output
                    .public_key_shares()
                    .iter()
                    .find(|key_share| key_share.participant() == pid);

                // Make sure every participant had a key share for this pid
                assert!(key_share.is_some());
                publics_for_pid.push(key_share.unwrap());
            }

            // Make sure they're all equal
            assert!(publics_for_pid.windows(2).all(|pks| pks[0] == pks[1]));

            // Check that each participant fully completed its broadcast portion.
            if let Status::ParticipantCompletedBroadcast(participants) =
                party.broadcast_participant().status()
            {
                assert_eq!(participants.len(), party.other_ids().len());
            } else {
                panic!("Broadcast not completed!");
            }
        }

        // Check that each participant's own `PublicKeyshare` corresponds to their
        // `PrivateKeyshare`
        for (output, pid) in outputs
            .iter()
            .zip(quorum.iter().map(ProtocolParticipant::id))
        {
            let public_share = output
                .public_key_shares()
                .iter()
                .find(|public_share| public_share.participant() == pid);
            assert!(public_share.is_some());

            let expected_public_share =
                TestCurve::GENERATOR.multiply_by_bignum(output.private_key_share().as_ref())?;
            assert_eq!(public_share.unwrap().as_ref(), &expected_public_share);
        }

        for (input, output) in inputs.iter().zip(outputs.iter()) {
            // The public key of the quorum has not changed.
            let pk_before = input.keygen_output().public_key()?;
            let pk_after = output.public_key()?;
            assert_eq!(pk_before, pk_after);

            // All key shares have changed.
            let public_shares_before = input
                .public_key_shares()
                .iter()
                .map(|key_share| serialize!(key_share.as_ref()).unwrap())
                .collect::<HashSet<_>>();

            let public_shares_after = output
                .public_key_shares()
                .iter()
                .map(|key_share| serialize!(key_share.as_ref()).unwrap())
                .collect::<HashSet<_>>();

            public_shares_before
                .intersection(&public_shares_after)
                .for_each(|_| {
                    panic!("All public key shares must change.");
                });

            // The rid has not changed.
            assert_eq!(input.keygen_output().rid(), output.rid());
        }

        Ok(())
    }
}
